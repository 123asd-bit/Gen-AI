import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
text = """
artificial intelligence is transforming modern society.
it is used in healthcare finance education and transportation.
machine learning allows systems to improve automatically with experience.
data plays a critical role in training intelligent systems.
large datasets help models learn complex patterns.
deep learning uses multi layer neural networks.
neural networks are inspired by biological neurons.
training a neural network requires optimization techniques.
gradient descent minimizes the loss function.
natural language processing helps computers understand human language.
text generation is a key task in nlp.
language models predict the next word or character.
lstm and gru models address long term dependency problems.
transformer models changed the field of nlp.
they rely on self attention mechanisms.
transformers process data in parallel.
this makes training faster and more efficient.
education is being improved using artificial intelligence.
intelligent tutoring systems personalize learning.
ethical considerations are important in artificial intelligence.
"""
tokenizer = Tokenizer()
tokenizer.fit_on_texts([text])
total_words = len(tokenizer.word_index) + 1

sequences = []
for line in text.split("\n"):
    token_list = tokenizer.texts_to_sequences([line])[0]
    for i in range(1, len(token_list)):
        sequences.append(token_list[:i+1])

max_len = max(len(seq) for seq in sequences)
sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')

X = sequences[:, :-1]
y = sequences[:, -1]
y = tf.keras.utils.to_categorical(y, num_classes=total_words)
model = Sequential([
    Embedding(total_words, 64),
    LSTM(128),
    Dropout(0.2),
    Dense(total_words, activation='softmax')
])

# ðŸ”¥ Force model to build
model.build(input_shape=(None, max_len-1))

model.compile(
    loss='categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)

model.summary()
history = model.fit(X, y, epochs=100, verbose=1)
def generate_text(seed_text, next_words=20):
    for _ in range(next_words):
        token_list = tokenizer.texts_to_sequences([seed_text])[0]
        token_list = pad_sequences([token_list], maxlen=max_len-1, padding='pre')
        predicted = np.argmax(model.predict(token_list), axis=-1)
        output_word = tokenizer.index_word[predicted[0]]
        seed_text += " " + output_word
    return seed_text

print(generate_text("artificial intelligence", 30))
import tensorflow as tf
from tensorflow.keras.layers import Layer, Dense, Dropout, Embedding, LayerNormalization
def positional_encoding(position, d_model):
    angles = np.arange(position)[:, None] / np.power(10000, (2*(np.arange(d_model)//2))/d_model)
    angles[:, 0::2] = np.sin(angles[:, 0::2])
    angles[:, 1::2] = np.cos(angles[:, 1::2])
    return tf.cast(angles[None, ...], dtype=tf.float32)
class TransformerBlock(Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):
        super().__init__()
        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = tf.keras.Sequential([
            Dense(ff_dim, activation="relu"),
            Dense(embed_dim),
        ])
        self.layernorm1 = LayerNormalization()
        self.layernorm2 = LayerNormalization()
        self.dropout1 = Dropout(rate)
        self.dropout2 = Dropout(rate)

    def call(self, inputs, training):
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)
# Combine entire text into one sequence
tokens = tokenizer.texts_to_sequences([text])[0]

seq_len = 10

input_sequences = []

for i in range(len(tokens) - seq_len):
    input_sequences.append(tokens[i:i+seq_len+1])

input_sequences = np.array(input_sequences)

print("Total sequences:", input_sequences.shape)

X_t = input_sequences[:, :-1]
y_t = input_sequences[:, -1]
X_t = input_sequences[:, :-1]
y_t = input_sequences[:, -1]

print(X_t.shape)
print(y_t.shape)
import tensorflow as tf
from tensorflow.keras.layers import Dense, Dropout, Embedding, LayerNormalization
from tensorflow.keras.layers import MultiHeadAttention, GlobalAveragePooling1D, Input
from tensorflow.keras.models import Model
import numpy as np

# Positional Encoding
def positional_encoding(position, d_model):
    angles = np.arange(position)[:, None] / np.power(
        10000, (2*(np.arange(d_model)[None, :]//2))/d_model
    )
    angles[:, 0::2] = np.sin(angles[:, 0::2])
    angles[:, 1::2] = np.cos(angles[:, 1::2])
    return tf.cast(angles[None, ...], dtype=tf.float32)

# Transformer Block
class TransformerBlock(tf.keras.layers.Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):
        super().__init__()
        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = tf.keras.Sequential([
            Dense(ff_dim, activation="relu"),
            Dense(embed_dim)
        ])
        self.norm1 = LayerNormalization()
        self.norm2 = LayerNormalization()
        self.drop1 = Dropout(rate)
        self.drop2 = Dropout(rate)

    def call(self, inputs):
        attn_output = self.att(inputs, inputs)
        out1 = self.norm1(inputs + self.drop1(attn_output))
        ffn_output = self.ffn(out1)
        return self.norm2(out1 + self.drop2(ffn_output))

# Hyperparameters
embed_dim = 64
num_heads = 2
ff_dim = 128
seq_len = 10

# Model Architecture
inputs = Input(shape=(seq_len,))
x = Embedding(total_words, embed_dim)(inputs)
x = x + positional_encoding(seq_len, embed_dim)
x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)
x = GlobalAveragePooling1D()(x)
x = Dense(64, activation="relu")(x)
outputs = Dense(total_words, activation="softmax")(x)

transformer_model = Model(inputs, outputs)

transformer_model.compile(
    optimizer="adam",
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

transformer_model.summary()
transformer_model.fit(X_t, y_t, epochs=50, batch_size=16)
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np

def generate_text_transformer(seed_text, words=20):
    for _ in range(words):
        token_list = tokenizer.texts_to_sequences([seed_text])[0]
        token_list = pad_sequences([token_list], maxlen=10, padding='pre')
        predicted = np.argmax(transformer_model.predict(token_list), axis=-1)
        seed_text += " " + tokenizer.index_word[predicted[0]]
    return seed_text

print(generate_text_transformer("artificial intelligence", 25))
