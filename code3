import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers, Model
from tensorflow.keras.datasets import mnist
# Load MNIST dataset
(x_train, _), (x_test, _) = mnist.load_data()

# Normalize to range [0,1]
x_train = x_train.astype("float32") / 255.0
x_test = x_test.astype("float32") / 255.0

# Flatten images (28x28 â†’ 784)
x_train = x_train.reshape(-1, 784)
x_test = x_test.reshape(-1, 784)

print("Training data shape:", x_train.shape)
print("Testing data shape:", x_test.shape)
input_dim = 784
hidden_dim = 256
latent_dim = 2
inputs = layers.Input(shape=(input_dim,))
h = layers.Dense(hidden_dim, activation="relu")(inputs)

z_mean = layers.Dense(latent_dim)(h)
z_log_var = layers.Dense(latent_dim)(h)
def sampling(args):
    z_mean, z_log_var = args
    epsilon = tf.random.normal(shape=tf.shape(z_mean))
    return z_mean + tf.exp(0.5 * z_log_var) * epsilon

z = layers.Lambda(sampling)([z_mean, z_log_var])
latent_inputs = layers.Input(shape=(latent_dim,))
h_dec = layers.Dense(hidden_dim, activation="relu")(latent_inputs)
outputs = layers.Dense(input_dim, activation="sigmoid")(h_dec)

decoder = Model(latent_inputs, outputs, name="decoder")
class VAE(tf.keras.Model):
    def __init__(self, encoder, decoder):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder

    def call(self, inputs):
        z_mean, z_log_var, z = self.encoder(inputs)
        return self.decoder(z)

    def train_step(self, data):
        if isinstance(data, tuple):
            data = data[0]

        with tf.GradientTape() as tape:
            z_mean, z_log_var, z = self.encoder(data)
            reconstruction = self.decoder(z)

            recon_loss = tf.reduce_mean(
                tf.keras.losses.binary_crossentropy(data, reconstruction)
            ) * input_dim

            kl_loss = -0.5 * tf.reduce_mean(
                1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)
            )

            total_loss = recon_loss + kl_loss

        grads = tape.gradient(total_loss, self.trainable_variables)
        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))

        return {
            "loss": total_loss,
            "reconstruction_loss": recon_loss,
            "kl_loss": kl_loss
        }
vae = VAE(encoder, decoder)
vae.compile(optimizer="adam")
history = vae.fit(
    x_train,
    epochs=30,
    batch_size=128
)
plt.plot(history.history["reconstruction_loss"])
plt.plot(history.history["kl_loss"])
plt.legend(["Reconstruction Loss", "KL Loss"])
plt.title("Loss Components")
plt.xlabel("Epoch")
plt.ylabel("Value")
plt.show()
n = 10
test_images = x_test[:n]
reconstructed = vae.predict(test_images)

plt.figure(figsize=(20,4))
for i in range(n):
    plt.subplot(2, n, i+1)
    plt.imshow(test_images[i].reshape(28,28), cmap="gray")
    plt.axis("off")

    plt.subplot(2, n, i+n+1)
    plt.imshow(reconstructed[i].reshape(28,28), cmap="gray")
    plt.axis("off")

plt.show()
random_latent_vectors = np.random.normal(size=(10, latent_dim))
generated_images = decoder.predict(random_latent_vectors)

plt.figure(figsize=(20,4))
for i in range(10):
    plt.subplot(1,10,i+1)
    plt.imshow(generated_images[i].reshape(28,28), cmap="gray")
    plt.axis("off")
plt.show()
z_mean, _, _ = encoder.predict(x_test)

plt.figure(figsize=(8,6))
plt.scatter(z_mean[:,0], z_mean[:,1], s=2)
plt.xlabel("Latent Dimension 1")
plt.ylabel("Latent Dimension 2")
plt.title("Latent Space Distribution")
plt.show()
